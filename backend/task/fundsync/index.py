import logging
import os
import time

import akshare as ak
from sqlalchemy import create_engine, text

# å¯¼å…¥è¿æ¥æ± ç®¡ç†å™¨
from common.db_pool import get_db_engine, get_db_connection, get_db_transaction

from .data_processor import FundDataProcessor
from .db_operations import FundDatabase
from common.config import FUND_CONFIG as DB_CONFIG

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s %(threadName)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def get_engine():
    """è·å–æ•°æ®åº“è¿æ¥å¼•æ“ï¼ˆä¼˜å…ˆä½¿ç”¨è¿æ¥æ± ï¼‰"""
    if get_db_engine is not None:
        # ä½¿ç”¨è¿æ¥æ± ç®¡ç†å™¨
        return get_db_engine()
    else:
        # é™çº§åˆ°åŸæœ‰æ–¹å¼
        return _create_legacy_engine()

def _create_legacy_engine():
    """åˆ›å»ºä¼ ç»Ÿæ•°æ®åº“å¼•æ“ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    user = os.environ.get("PG_USER")
    password = os.environ.get("PG_PASSWORD")
    host = os.environ.get("PG_HOST_OUT") or os.environ.get("PG_HOST")    
    port = os.environ.get("PG_PORT", "5432")
    dbname = os.environ.get("PG_DB")
    
    if not all([user, password, host, dbname]):
        logger.error("ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥PG_USER, PG_PASSWORD, PG_HOST_OUT, PG_DB")
        return None
        
    conn_str = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}"
    engine = create_engine(conn_str, echo=False, future=True)
    return engine   

def sync_task_runner():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡ŒåŸºé‡‘æ•°æ®åŒæ­¥"""
    start_time = time.time()
    
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å’Œå¤„ç†å™¨
    engine = get_engine()
    if engine is None:
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°åˆå§‹åŒ–æ•°æ®åº“æ“ä½œç±»
    db = FundDatabase(
        engine, 
        max_workers=DB_CONFIG["max_workers"], 
        batch_size=DB_CONFIG["batch_size"]
    )
    processor = FundDataProcessor()
    
    logger.info(f"ğŸ”§ ç³»ç»Ÿé…ç½®:")
    logger.info(f"   - æ•°æ®æ¥å£è°ƒç”¨: ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…APIé™åˆ¶ï¼‰")
    logger.info(f"   - æ•°æ®åº“æ“ä½œ: {DB_CONFIG['max_workers']} çº¿ç¨‹å¹¶å‘")
    logger.info(f"   - æ‰¹æ¬¡å¤§å°: {DB_CONFIG['batch_size']} æ¡/æ‰¹")
    logger.info(f"   - é‡è¯•æ¬¡æ•°: {DB_CONFIG['max_retry']} æ¬¡")
    logger.info(f"   - é‡è¯•å»¶è¿Ÿ: {DB_CONFIG['retry_delay']} ç§’")
    logger.info("å¼€å§‹åŸºé‡‘æ•°æ®åŒæ­¥ä»»åŠ¡")
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åŸºé‡‘æ•°æ®
    fund_data = processor.collect_all_fund_data()
    if not fund_data:
        logger.error("âŒ åŸºé‡‘æ•°æ®è·å–ä¸å®Œæ•´ï¼Œä»»åŠ¡ç»ˆæ­¢ï¼Œä¸ä¼šä¸æ•°æ®åº“äº¤äº’")
        logger.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIæ¥å£çŠ¶æ€ï¼Œç¡®ä¿æ‰€æœ‰äº”ä¸ªæ•°æ®æºéƒ½èƒ½æ­£å¸¸è®¿é—®")
        return
    
    logger.info(f"âœ… æ‰€æœ‰åŸºé‡‘æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(fund_data)} ä¸ªç±»å‹")
    
    # ç¬¬äºŒæ­¥ï¼šæ•´åˆåŸºé‡‘æ•°æ®ï¼Œå¤„ç†é‡å¤ç¼–ç 
    final_funds, _ = processor.integrate_fund_data(fund_data)
    
    # ç¬¬ä¸‰æ­¥ï¼šè·å–æ•°æ®åº“ç°æœ‰æ•°æ®å¹¶åˆ†æå·®å¼‚
    existing_codes = db.get_existing_fund_codes()
    new_codes, removed_codes, update_codes = processor.analyze_data_differences(final_funds, existing_codes)
    
    # ç¬¬å››æ­¥ï¼šæ‰§è¡Œæ•°æ®åº“æ›´æ–°æ“ä½œ
    logger.info("å¼€å§‹æ‰§è¡Œæ•°æ®åº“æ›´æ–°æ“ä½œ...")
    db.execute_batch_updates(final_funds, new_codes, removed_codes, update_codes)
    
    total_time = time.time() - start_time
    logger.info(f"åŸºé‡‘æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.4f} ç§’")

if __name__ == "__main__":
    sync_task_runner()



def handler(event, context):
    logger.warning("æ”¶åˆ°è°ƒç”¨è¯·æ±‚ï¼Œå¼€å§‹åŒæ­¥æ‰§è¡Œæ‰¹é‡æ›´æ–°ä»»åŠ¡...")
    sync_task_runner()
    logger.warning("åŒæ­¥ä»»åŠ¡å®Œæˆï¼Œå‡†å¤‡è¿”å›ã€‚")
    return "æ‰¹é‡æ›´æ–°ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œæ—¥å¿—å·²å®Œæ•´è¾“å‡º"