#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºé‡‘æ•°æ®å¤„ç†æ¨¡å—
"""

import logging
import time
from typing import Dict, List, Set, Tuple, Optional, Any

import akshare as ak
import pandas as pd
from requests.exceptions import ChunkedEncodingError

# å¯¼å…¥æŠ½è±¡åŸºç±»å’Œå·¥å‚æ¨¡å¼
from common.base_processor import BaseDataProcessor
from common.data_factory import create_data_factory, FundDataFactory

# å¯¼å…¥è¿æ¥æ± ç®¡ç†å™¨
from common.db_pool import get_db_engine, get_db_connection, get_db_transaction

# å¯¼å…¥é…ç½®
from common.config import FUND_TYPES, FUND_CONFIG as DB_CONFIG

logger = logging.getLogger(__name__)

class FundDataProcessor(BaseDataProcessor):
    """åŸºé‡‘æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__()
        
        # ä½¿ç”¨å·¥å‚æ¨¡å¼åˆå§‹åŒ–
        self.data_factory = create_data_factory('fund')
        
        # åŠ¨æ€å¯¼å…¥akshareå‡½æ•°
        self.fund_tasks = []
        self.log_section_start("ğŸ”§ å¼€å§‹åˆå§‹åŒ–åŸºé‡‘æ•°æ®å¤„ç†å™¨")
        logger.info("âœ… ä½¿ç”¨å·¥å‚æ¨¡å¼é…ç½®")
        self.log_section_end("åŸºé‡‘æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        for fund_type, config in FUND_TYPES.items():
            try:
                api_func_name = config["api_func"]
                logger.info(f"ğŸ“‹ åŸºé‡‘ç±»å‹ '{fund_type}' é…ç½®: {api_func_name}")
                
                # ç›´æ¥ä½¿ç”¨akshareæ¨¡å—çš„å‡½æ•°
                if hasattr(ak, api_func_name):
                    func = getattr(ak, api_func_name)
                    logger.info(f"âœ… æˆåŠŸè·å–å‡½æ•°: {api_func_name}")
                    self.fund_tasks.append((fund_type, func, config["concurrency"]))
                else:
                    logger.error(f"âŒ æœªæ‰¾åˆ°å‡½æ•°: {api_func_name} åœ¨ akshare æ¨¡å—ä¸­")
                    available_funcs = [attr for attr in dir(ak) if not attr.startswith('_')]
                    logger.info(f"ğŸ’¡ å¯ç”¨çš„ akshare å‡½æ•°ç¤ºä¾‹: {available_funcs[:10]}...")
                    logger.info(f"ğŸ’¡ æ€»å…± {len(available_funcs)} ä¸ªå¯ç”¨å‡½æ•°")
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–åŸºé‡‘ç±»å‹ '{fund_type}' å¤±è´¥: {e}", exc_info=True)
        
        logger.info("-" * 40)
        logger.info(f"ğŸ¯ åŸºé‡‘æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š æˆåŠŸé…ç½®: {len(self.fund_tasks)}/{len(FUND_TYPES)} ä¸ªåŸºé‡‘ç±»å‹")
        
        if self.fund_tasks:
            logger.info("âœ… å·²é…ç½®çš„åŸºé‡‘ç±»å‹:")
            for fund_type, func, concurrency in self.fund_tasks:
                logger.info(f"   - {fund_type}: {func.__name__} (å¹¶å‘æ•°: {concurrency})")
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸé…ç½®ä»»ä½•åŸºé‡‘ç±»å‹ï¼")
            
        logger.info("=" * 60)
    
    def safe_get_data(self, get_func, fund_name: str, max_retry=None) -> Optional:
        """å®‰å…¨è·å–æ•°æ®ï¼Œå¸¦å¢å¼ºçš„é‡è¯•æœºåˆ¶å’Œé”™è¯¯åˆ†ç±»"""
        if max_retry is None:
            max_retry = DB_CONFIG["max_retry"]
        
        # è·å–å‡½æ•°åç§°ç”¨äºæ—¥å¿—
        func_name = get_func.__name__ if hasattr(get_func, '__name__') else str(get_func)
        logger.info(f"[{fund_name}] å¼€å§‹è°ƒç”¨å‡½æ•°: {func_name}")
            
        for attempt in range(max_retry):
            try:
                logger.info(f"[{fund_name}] ç¬¬ {attempt+1} æ¬¡å°è¯•è°ƒç”¨ {func_name}")
                df = get_func()
                logger.info(f"[{fund_name}] {func_name} è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ•°æ®ç±»å‹: {type(df)}")
                
                if df is not None and not df.empty:
                    logger.info(f"[{fund_name}] {func_name} è¿”å›æ•°æ®è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns) if hasattr(df, 'columns') else 'N/A'}")
                    return df
                else:
                    logger.warning(f"[{fund_name}] {func_name} è¿”å›æ•°æ®ä¸ºç©ºæˆ–Noneï¼Œç¬¬ {attempt+1} æ¬¡é‡è¯•")
            except Exception as e:
                # é”™è¯¯åˆ†ç±»å¤„ç†
                error_type = self._classify_error(e)
                logger.error(f"âŒ [{fund_name}] ç¬¬ {attempt + 1} æ¬¡è·å–æ•°æ®å¤±è´¥ [{error_type}]: {e}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
                if not self._should_retry(error_type, attempt, max_retry):
                    logger.error(f"ğŸ’¥ [{fund_name}] é”™è¯¯ç±»å‹ {error_type} ä¸é€‚åˆé‡è¯•ï¼Œç›´æ¥å¤±è´¥")
                    return None
                
                if attempt < max_retry - 1:
                    # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é‡è¯•å»¶è¿Ÿ
                    adjusted_delay = self._get_retry_delay(error_type, DB_CONFIG["retry_delay"], attempt)
                    logger.info(f"â³ [{fund_name}] {adjusted_delay} ç§’åé‡è¯•...")
                    time.sleep(adjusted_delay)
                else:
                    logger.error(f"ğŸ’¥ [{fund_name}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
                    return None
        
        logger.error(f"[{fund_name}] {func_name} æ‹‰å–æ•°æ®å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")
        return None
    
    # é”™è¯¯å¤„ç†æ–¹æ³•å·²ç§»è‡³åŸºç±» BaseDataProcessor
    
    def get_price_column(self, fund_type: str, df_columns) -> Optional[str]:
        """æ ¹æ®åŸºé‡‘ç±»å‹è·å–ä»·æ ¼åˆ—å"""
        config = FUND_TYPES.get(fund_type, {})
        pattern = config.get("price_column_pattern", "")
        
        if pattern == "ç°ä»·":
            return "ç°ä»·" if "ç°ä»·" in df_columns else None
        else:
            return next((c for c in df_columns if pattern in c), None)
    
    def extract_fund_values(self, df, fund_type: str) -> List[Tuple]:
        """æå–åŸºé‡‘æ•°æ®"""
        records = []
        price_col = self.get_price_column(fund_type, df.columns)
        
        if not price_col:
            logger.warning(f"[{fund_type}] æœªæ‰¾åˆ°ä»·æ ¼åˆ—")
            return records
        
        # è¿‡æ»¤ä»·æ ¼ä¸ä¸ºç©ºçš„æ•°æ®
        filtered_df = df[df[price_col].notna()]
        logger.info(f"[{fund_type}] ä»·æ ¼ä¸ä¸ºç©ºçš„æ•°æ®æ¡æ•°: {len(filtered_df)}")
        
        for _, row in filtered_df.iterrows():
            code = row.get("åŸºé‡‘ä»£ç ") or row.get("ä»£ç ") or ""
            name = row.get("åŸºé‡‘ç®€ç§°") or row.get("åç§°") or ""
            price = self.safe_float(row.get(price_col, None))
            if code and name:  # ç¡®ä¿ç¼–ç å’Œåç§°ä¸ä¸ºç©º
                records.append((code, name, fund_type, price))
        
        return records
    
    def safe_float(self, val) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        try:
            return float(val)
        except Exception:
            return None
    
    def collect_all_fund_data(self) -> Optional[Dict[str, List[Tuple]]]:
        """æ”¶é›†æ‰€æœ‰åŸºé‡‘æ•°æ® - å¿…é¡»å…¨éƒ¨æˆåŠŸæ‰èƒ½ç»§ç»­"""
        fund_data = {}
        required_types = set(FUND_TYPES.keys())
        success_types = set()
        failed_types = []
        
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹æ”¶é›†åŸºé‡‘æ•°æ®")
        logger.info(f"ğŸ“‹ å¿…é¡»è·å–å…¨éƒ¨ {len(required_types)} ä¸ªç±»å‹: {list(required_types)}")
        logger.info("=" * 60)
        
        for fund_name, get_func, concurrency in self.fund_tasks:
            logger.info(f"ğŸ“¥ æ­£åœ¨è·å–: {fund_name}")
            df = self.safe_get_data(get_func, fund_name)
            
            if df is None or df.empty:
                logger.error(f"âŒ [{fund_name}] æ•°æ®è·å–å¤±è´¥æˆ–ä¸ºç©º")
                failed_types.append(fund_name)
                continue
            
            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—å¤„ç†å¤§æ•°æ®é›†
            chunk_size = 5000  # åŸºé‡‘æ•°æ®ç›¸å¯¹è¾ƒå°ï¼Œä½¿ç”¨5000æ¡è®°å½•ä¸ºä¸€å—
            total_rows = len(df)
            
            if total_rows > chunk_size:
                logger.info(f"ğŸ“¦ [{fund_name}] å¤§æ•°æ®é›†æ£€æµ‹ï¼Œå¯ç”¨åˆ†å—å¤„ç†æ¨¡å¼ (chunk_size={chunk_size})")
                
                all_records = []
                # åˆ†å—å¤„ç†
                for i in range(0, total_rows, chunk_size):
                    chunk_end = min(i + chunk_size, total_rows)
                    chunk_df = df.iloc[i:chunk_end]
                    
                    logger.info(f"ğŸ“¦ [{fund_name}] å¤„ç†æ•°æ®å— {i//chunk_size + 1}/{(total_rows-1)//chunk_size + 1} ({i+1}-{chunk_end})")
                    
                    chunk_records = self.extract_fund_values(chunk_df, fund_name)
                    all_records.extend(chunk_records)
                    
                    # æ¸…ç†ä¸´æ—¶å˜é‡
                    del chunk_df, chunk_records
                
                records = all_records
                del all_records
            else:
                # å°æ•°æ®é›†ç›´æ¥å¤„ç†
                records = self.extract_fund_values(df, fund_name)
            
            # æ¸…ç†DataFrame
            del df
            
            if records:
                fund_data[fund_name] = records
                success_types.add(fund_name)
                logger.info(f"âœ… [{fund_name}] æˆåŠŸè·å– {len(records)} æ¡è®°å½•")
            else:
                logger.error(f"âŒ [{fund_name}] æå–æ•°æ®å¤±è´¥ï¼Œæ— æœ‰æ•ˆè®°å½•")
                failed_types.append(fund_name)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç±»å‹éƒ½æˆåŠŸè·å–
        logger.info("-" * 40)
        logger.info("ğŸ“Š æ•°æ®æ”¶é›†ç»“æœç»Ÿè®¡")
        logger.info("-" * 40)
        
        if success_types == required_types:
            logger.info(f"ğŸ‰ æ‰€æœ‰åŸºé‡‘ç±»å‹æ•°æ®è·å–æˆåŠŸï¼")
            logger.info(f"âœ… æˆåŠŸç±»å‹: {list(success_types)} ({len(success_types)}/{len(required_types)})")
            logger.info("=" * 60)
            return fund_data
        else:
            missing_types = required_types - success_types
            logger.error(f"âŒ åŸºé‡‘æ•°æ®è·å–ä¸å®Œæ•´ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            logger.error(f"âœ… æˆåŠŸè·å–: {list(success_types)} ({len(success_types)}/{len(required_types)})")
            logger.error(f"âŒ è·å–å¤±è´¥: {list(missing_types)}")
            logger.error(f"ğŸ“ å¤±è´¥è¯¦æƒ…: {failed_types}")
            logger.error("ğŸ›‘ ç”±äºæ•°æ®ä¸å®Œæ•´ï¼Œå°†ç»ˆæ­¢åç»­æ“ä½œ")
            logger.error("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIæ¥å£çŠ¶æ€")
            logger.info("=" * 60)
            return None
    
    def integrate_fund_data(self, fund_data: Dict[str, List[Tuple]]) -> Tuple[List[Tuple], Set[str]]:
        """æ•´åˆåŸºé‡‘æ•°æ®ï¼Œå¤„ç†é‡å¤ç¼–ç """
        logger.info("=" * 60)
        logger.info("ğŸ”„ å¼€å§‹æ•´åˆåŸºé‡‘æ•°æ®ï¼Œå¤„ç†é‡å¤ç¼–ç ")
        logger.info("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆå¹¶éQDIIåŸºé‡‘æ•°æ®ï¼Œå¹¶å»é™¤ä»·æ ¼ä¸ºç©ºçš„æ•°æ®
        domestic_funds = []
        domestic_codes = set()
        domestic_type_counts = {}  # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        
        empty_price_codes = set()  # æ”¶é›†ä»·æ ¼ä¸ºç©ºçš„ç¼–ç 
        
        for fund_type in ["åœºå†…åŸºé‡‘", "å¼€æ”¾åŸºé‡‘", "è´§å¸åŸºé‡‘"]:
            if fund_type in fund_data:
                type_records = fund_data[fund_type]
                logger.info(f"ğŸ“Š {fund_type}: {len(type_records)} æ¡è®°å½•")
                
                type_count = 0
                for record in type_records:
                    code, name, _, price = record
                    if price is not None:  # åªä¿ç•™ä»·æ ¼ä¸ä¸ºç©ºçš„æ•°æ®
                        domestic_funds.append((code, name, fund_type, price))
                        domestic_codes.add(code)
                        type_count += 1
                    else:
                        empty_price_codes.add(code)
                
                domestic_type_counts[fund_type] = type_count
                
                # å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†å·²å¤„ç†çš„æ•°æ®
                del fund_data[fund_type]
        
        # ç»Ÿä¸€æ‰“å°ä»·æ ¼ä¸ºç©ºçš„ç¼–ç é›†åˆ
        if empty_price_codes:
            logger.info(f"ğŸ”„ æ’é™¤ä»·æ ¼ä¸ºç©ºçš„åŸºé‡‘ç¼–ç é›†åˆ ({len(empty_price_codes)} ä¸ª): {sorted(empty_price_codes)}")
        
        logger.info(f"ğŸ“Š éQDIIåŸºé‡‘ï¼ˆä»·æ ¼ä¸ä¸ºç©ºï¼‰: {len(domestic_funds)} æ¡")
        logger.info(f"ğŸ“Š éQDIIåŸºé‡‘ç¼–ç é›†åˆ: {len(domestic_codes)} ä¸ª")
        logger.info("ğŸ“‹ éQDIIåŸºé‡‘å„ç±»å‹ç»Ÿè®¡:")
        for fund_type, count in domestic_type_counts.items():
            logger.info(f"     {fund_type}: {count} æ¡")
        
        # ç¬¬äºŒæ­¥ï¼šQDIIåŸºé‡‘ä½œä¸ºè¡¥å……ï¼Œåªæ·»åŠ ä¸å­˜åœ¨çš„åŸºé‡‘
        qdii_funds = []
        qdii_supplemented = 0
        qdii_skipped = 0
        qdii_type_counts = {}  # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        
        for fund_type in ["æ¬§ç¾QDII", "äºšæ´²QDII"]:
            if fund_type in fund_data:
                type_records = fund_data[fund_type]
                logger.info(f"ğŸ“Š {fund_type}: {len(type_records)} æ¡è®°å½•")
                
                type_supplemented = 0
                type_skipped = 0
                
                for record in type_records:
                    code, name, _, price = record
                    if code not in domestic_codes:
                        # ç¼–ç ä¸å­˜åœ¨ï¼Œæ·»åŠ è¿™æ¡QDIIåŸºé‡‘
                        qdii_funds.append((code, name, fund_type, price))
                        domestic_codes.add(code)
                        qdii_supplemented += 1
                        type_supplemented += 1
                        logger.info(f"â• è¡¥å……QDIIåŸºé‡‘: {code} ({name}) - {fund_type}")
                    else:
                        # ç¼–ç å·²å­˜åœ¨ï¼Œè·³è¿‡è¿™æ¡QDIIåŸºé‡‘
                        qdii_skipped += 1
                        type_skipped += 1
                        logger.info(f"â­ï¸  è·³è¿‡QDIIåŸºé‡‘: {code} ({name}) - {fund_type} (ç¼–ç å·²å­˜åœ¨)")
                
                qdii_type_counts[fund_type] = {
                    'supplemented': type_supplemented,
                    'skipped': type_skipped
                }
                
                # å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†å·²å¤„ç†çš„æ•°æ®
                del fund_data[fund_type]
        
        logger.info(f"ğŸ“Š QDIIåŸºé‡‘è¡¥å……: {qdii_supplemented} æ¡")
        logger.info(f"ğŸ“Š QDIIåŸºé‡‘è·³è¿‡: {qdii_skipped} æ¡")
        logger.info("ğŸ“‹ QDIIåŸºé‡‘å„ç±»å‹ç»Ÿè®¡:")
        for fund_type, counts in qdii_type_counts.items():
            logger.info(f"     {fund_type}: è¡¥å…… {counts['supplemented']} æ¡, è·³è¿‡ {counts['skipped']} æ¡")
        
        # æœ€ç»ˆé›†åˆ
        final_funds = domestic_funds + qdii_funds
        
        # ç»Ÿè®¡å„ç±»å‹åŸºé‡‘æ•°é‡
        type_counts = {}
        for fund in final_funds:
            fund_type = fund[2]  # fund[2]æ˜¯åŸºé‡‘ç±»å‹
            type_counts[fund_type] = type_counts.get(fund_type, 0) + 1
        
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ æ•°æ®æ•´åˆå®Œæˆï¼æœ€ç»ˆåŸºé‡‘é›†åˆ: {len(final_funds)} æ¡")
        logger.info(f"   - éQDIIåŸºé‡‘: {len(domestic_funds)} æ¡")
        logger.info(f"   - QDIIè¡¥å……åŸºé‡‘: {len(qdii_funds)} æ¡")
        logger.info(f"   - æ€»ç¼–ç æ•°: {len(domestic_codes)} ä¸ª")
        
        # æ˜¾ç¤ºå„ç±»å‹åŸºé‡‘è¯¦ç»†ç»Ÿè®¡
        logger.info("ğŸ“Š å„ç±»å‹åŸºé‡‘è¯¦ç»†ç»Ÿè®¡:")
        for fund_type, count in sorted(type_counts.items()):
            logger.info(f"     {fund_type}: {count} æ¡")
        
        logger.info("=" * 60)
        
        return final_funds, domestic_codes
    
    def analyze_data_differences(self, final_funds: List[Tuple], existing_codes: Set[str]) -> Tuple[Set[str], Set[str], Set[str]]:
        """åˆ†ææ•°æ®å·®å¼‚ï¼Œè¿”å›æ–°å¢ã€åˆ é™¤ã€æ›´æ–°çš„åŸºé‡‘ç¼–ç é›†åˆ"""
        self.log_section_start("ğŸ” å¼€å§‹åˆ†ææ•°æ®å·®å¼‚")
        
        final_codes = {f[0] for f in final_funds}
        new_codes = final_codes - existing_codes  # æ–°å¢ï¼šåœ¨æœ€ç»ˆæ•°æ®ä¸­ä½†ä¸åœ¨æ•°æ®åº“ä¸­
        removed_codes = existing_codes - final_codes  # åˆ é™¤ï¼šåœ¨æ•°æ®åº“ä¸­ä½†ä¸åœ¨æœ€ç»ˆæ•°æ®ä¸­
        update_codes = final_codes & existing_codes  # æ›´æ–°ï¼šæ—¢åœ¨æœ€ç»ˆæ•°æ®ä¸­ä¹Ÿåœ¨æ•°æ®åº“ä¸­
        
        logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        logger.info(f"   - æœ€ç»ˆåŸºé‡‘é›†åˆ: {len(final_funds)} æ¡")
        logger.info(f"   - æ•°æ®åº“ç°æœ‰: {len(existing_codes)} æ¡")
        logger.info(f"   - æ–°å¢åŸºé‡‘: {len(new_codes)} æ¡")
        logger.info(f"   - æ›´æ–°åŸºé‡‘: {len(update_codes)} æ¡")
        logger.info(f"   - éœ€è¦åˆ é™¤: {len(removed_codes)} æ¡")
        
        # è¾“å‡ºç¤ºä¾‹æ•°æ®
        if new_codes:
            logger.info(f"ğŸ†• æ–°å¢åŸºé‡‘ç¼–ç ç¤ºä¾‹: {list(new_codes)[:5]}")
            if len(new_codes) > 5:
                logger.info(f"   ... è¿˜æœ‰ {len(new_codes) - 5} ä¸ªç¼–ç ")
        else:
            logger.info("âœ… æ— æ–°å¢åŸºé‡‘ç¼–ç ")
            
        if update_codes:
            logger.info(f"ğŸ”„ æ›´æ–°åŸºé‡‘ç¼–ç ç¤ºä¾‹: {list(update_codes)[:5]}")
            if len(update_codes) > 5:
                logger.info(f"   ... è¿˜æœ‰ {len(update_codes) - 5} ä¸ªç¼–ç ")
        else:
            logger.info("âœ… æ— éœ€æ›´æ–°åŸºé‡‘ç¼–ç ")
            
        if removed_codes:
            logger.info(f"ğŸ—‘ï¸  éœ€è¦åˆ é™¤çš„åŸºé‡‘ç¼–ç ç¤ºä¾‹: {list(removed_codes)[:5]}")
            if len(removed_codes) > 5:
                logger.info(f"   ... è¿˜æœ‰ {len(removed_codes) - 5} ä¸ªç¼–ç ")
        else:
            logger.info("âœ… æ— éœ€åˆ é™¤åŸºé‡‘ç¼–ç ")
        
        return new_codes, removed_codes, update_codes
    
    def get_data_sources(self) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®æºé…ç½®"""
        sources = []
        for fund_type, func, concurrency in self.fund_tasks:
            config = FUND_TYPES.get(fund_type, {})
            sources.append({
                'name': fund_type,
                'api_func': func,
                'clean_func': self.extract_fund_values,
                'concurrency': concurrency,
                'batch_size': DB_CONFIG.get('batch_size', 1000)
            })
        return sources
    
    def process_data(self, data: Any, fund_type: str) -> Optional[List[Tuple]]:
        """å¤„ç†æ•°æ®"""
        # åŸºé‡‘æ•°æ®å¤„ç†é€»è¾‘å·²åœ¨extract_fund_valuesä¸­å®ç°
        # è¿™é‡Œè¿”å›åŸå§‹æ•°æ®ï¼Œå®é™…å¤„ç†åœ¨extract_fund_valuesä¸­è¿›è¡Œ
        return data
    
    def sync_to_database(self, data: List[Tuple], fund_type: str, db_connection) -> Dict[str, Any]:
        """åŒæ­¥æ•°æ®åˆ°æ•°æ®åº“"""
        # åŸºé‡‘æ•°æ®åŒæ­¥é€»è¾‘éœ€è¦åœ¨å…·ä½“å®ç°ä¸­å®Œæˆ
        # è¿™é‡Œè¿”å›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        return {
            "total_records": len(data),
            "fund_type": fund_type,
            "status": "success"
        }