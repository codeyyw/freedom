#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®å¤„ç†æ¨¡å—
"""

import logging
import os
import time
from typing import Dict, List, Set, Tuple, Optional, Any

import akshare as ak
import pandas as pd
from sqlalchemy import create_engine

# å¯¼å…¥æŠ½è±¡åŸºç±»å’Œå·¥å‚æ¨¡å¼
from common.base_processor import BaseDataProcessor
from common.data_factory import create_data_factory, StockDataFactory

# å¯¼å…¥è¿æ¥æ± ç®¡ç†å™¨
from common.db_pool import get_db_engine, get_db_connection, get_db_transaction

# å¯¼å…¥æ•°æ®åº“æ“ä½œæ¨¡å—
from .db_operations import (
    get_existing_stock_symbols_by_market,
    update_stock_records_batch,
    insert_stock_records_batch,
    mark_stocks_as_deleted,
)

# å¯¼å…¥å¸‚åœºçŠ¶æ€æ£€æŸ¥æ¨¡å—
from .market_status import is_us_stock_market_open, check_market_open_status, is_hk_stock_market_open, is_a_stock_market_open

# å¯¼å…¥è‚¡ç¥¨æ¸…æ´—æ¨¡å—
from .stock_cleaners import clean_a_stock_data, clean_hk_stock_data, clean_us_stock_data

# å¯¼å…¥é…ç½®
from common.config import STOCK_CONFIG as DB_CONFIG, RETRY_CONFIG

logger = logging.getLogger(__name__)

class StockDataProcessor(BaseDataProcessor):
    """è‚¡ç¥¨æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è‚¡ç¥¨æ•°æ®å¤„ç†å™¨"""
        super().__init__()
        
        self.log_section_start("ğŸ”§ å¼€å§‹åˆå§‹åŒ–è‚¡ç¥¨æ•°æ®å¤„ç†å™¨")
        
        # åˆå§‹åŒ–æ•°æ®å·¥å‚ï¼ˆå¿…é¡»æˆåŠŸï¼‰
        try:
            self.data_factory = create_data_factory("stock")
            if not self.data_factory:
                raise RuntimeError("æ•°æ®å·¥å‚åˆ›å»ºå¤±è´¥")
            logger.info("âœ… æ•°æ®å·¥å‚åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å·¥å‚åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RuntimeError(f"æ•°æ®å·¥å‚åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ä½¿ç”¨å·¥å‚æ¨¡å¼é…ç½®
        self.market_configs = self._create_market_configs_from_factory()
        logger.info("âœ… ä½¿ç”¨å·¥å‚æ¨¡å¼é…ç½®")
        
        logger.info("-" * 40)
        logger.info(f"ğŸ¯ è‚¡ç¥¨æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š æˆåŠŸé…ç½®: {len(self.market_configs)} ä¸ªå¸‚åœº")
        
        if self.market_configs:
            logger.info("âœ… å·²é…ç½®çš„å¸‚åœº:")
            for config in self.market_configs:
                market_name = config["market_name"]
                func_name = config["get_data_func"].__name__
                concurrency = config["concurrency"]
                logger.info(f"   - {market_name}: {func_name} (å¹¶å‘æ•°: {concurrency})")
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸé…ç½®ä»»ä½•å¸‚åœºï¼")
            
        self.log_section_end("è‚¡ç¥¨æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_market_configs_from_factory(self) -> List[Dict[str, Any]]:
        """ä»å·¥å‚åˆ›å»ºå¸‚åœºé…ç½®"""
        configs = []
        
        # å¸‚åœºå¼€ç›˜æ£€æŸ¥å‡½æ•°æ˜ å°„
        check_open_funcs = {
            "Aè‚¡": is_a_stock_market_open,
            "æ¸¯è‚¡": is_hk_stock_market_open,
            "ç¾è‚¡": is_us_stock_market_open
        }
        
        for source_config in self.data_factory.get_all_data_sources():
            config = {
                "market_name": source_config.name,
                "get_data_func": source_config.api_func,
                "clean_func": source_config.clean_func,
                "check_open_func": check_open_funcs.get(source_config.name),
                "concurrency": source_config.concurrency,
            }
            configs.append(config)
        
        return configs
    
    def get_data_sources(self) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®æºé…ç½®"""
        return self.market_configs
    
    def safe_get_data(self, get_func, source_name: str, **kwargs) -> Optional[Any]:
        """å®‰å…¨è·å–æ•°æ®"""
        try:
            if get_func is None:
                logger.warning(f"æ•°æ®æº {source_name} çš„è·å–å‡½æ•°ä¸ºç©º")
                return None
                
            # è°ƒç”¨æ•°æ®è·å–å‡½æ•°
            data = get_func(**kwargs)
            
            if data is None or (hasattr(data, 'empty') and data.empty):
                logger.warning(f"æ•°æ®æº {source_name} è¿”å›ç©ºæ•°æ®")
                return None
                
            logger.info(f"æˆåŠŸè·å– {source_name} æ•°æ®ï¼Œè®°å½•æ•°: {len(data) if hasattr(data, '__len__') else 'æœªçŸ¥'}")
            return data
            
        except Exception as e:
            logger.error(f"è·å– {source_name} æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def process_data(self, data: pd.DataFrame, market_name: str) -> pd.DataFrame:
        """å¤„ç†æ•°æ®"""
        # æ‰¾åˆ°å¯¹åº”å¸‚åœºçš„æ¸…æ´—å‡½æ•°
        for config in self.market_configs:
            if config["market_name"] == market_name:
                clean_func = config["clean_func"]
                return clean_func(data)
        
        raise ValueError(f"æœªæ‰¾åˆ°å¸‚åœº {market_name} çš„æ¸…æ´—å‡½æ•°")
    
    def sync_to_database(self, data: pd.DataFrame, market_name: str, db_connection) -> Dict[str, Any]:
        """åŒæ­¥æ•°æ®åˆ°æ•°æ®åº“"""
        return self.process_market_records(data, market_name, db_connection)
    
    def safely_get_market_data(self, get_data_func, market_name: str, max_retry: int = None):
        """å®‰å…¨è·å–å¸‚åœºæ•°æ®ï¼Œå¸¦å¢å¼ºçš„é‡è¯•æœºåˆ¶å’Œé”™è¯¯åˆ†ç±»"""
        if max_retry is None:
            max_retry = RETRY_CONFIG.get("max_retry", 3)
        
        retry_delay = RETRY_CONFIG.get("retry_delay", 2)
        timeout = RETRY_CONFIG.get("timeout", 30)
        
        for attempt in range(max_retry):
            try:
                logger.info(f"ğŸ”„ [{market_name}] ç¬¬ {attempt + 1}/{max_retry} æ¬¡å°è¯•è·å–æ•°æ®")
                
                # è®¾ç½®è¶…æ—¶
                import signal
                signal.alarm(timeout)
                
                data = get_data_func()
                
                # å–æ¶ˆè¶…æ—¶
                signal.alarm(0)
                
                # æ•°æ®éªŒè¯
                if data is None or len(data) == 0:
                    raise ValueError(f"è·å–åˆ°ç©ºæ•°æ®æˆ–æ— æ•ˆæ•°æ®")
                
                logger.info(f"âœ… [{market_name}] æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
                
            except Exception as e:
                signal.alarm(0)  # ç¡®ä¿å–æ¶ˆè¶…æ—¶
                
                # é”™è¯¯åˆ†ç±»å¤„ç†
                error_type = self._classify_error(e)
                logger.error(f"âŒ [{market_name}] ç¬¬ {attempt + 1} æ¬¡è·å–æ•°æ®å¤±è´¥ [{error_type}]: {e}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
                if not self._should_retry(error_type, attempt, max_retry):
                    logger.error(f"ğŸ’¥ [{market_name}] é”™è¯¯ç±»å‹ {error_type} ä¸é€‚åˆé‡è¯•ï¼Œç›´æ¥å¤±è´¥")
                    return None
                
                if attempt < max_retry - 1:
                    # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é‡è¯•å»¶è¿Ÿ
                    adjusted_delay = self._get_retry_delay(error_type, retry_delay, attempt)
                    logger.info(f"â³ [{market_name}] {adjusted_delay} ç§’åé‡è¯•...")
                    time.sleep(adjusted_delay)
                else:
                    logger.error(f"ğŸ’¥ [{market_name}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œæ”¾å¼ƒè·å–æ•°æ®")
        
        return None
    
    # é”™è¯¯å¤„ç†æ–¹æ³•å·²ç§»è‡³åŸºç±» BaseDataProcessor
    
    def format_duration(self, duration_sec: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if duration_sec >= 120:
            return f"{duration_sec / 60:.4f} åˆ†é’Ÿ"
        else:
            return f"{duration_sec:.4f} ç§’"
    
    def process_and_sync_stock_data(
        self,
        market_name: str,
        get_data_func,
        clean_func,
        existing_symbols: Set[str],
        concurrency: int = 2,
        batch_size: int = None,
    ):
        """å¤„ç†å¹¶åŒæ­¥è‚¡ç¥¨æ•°æ®"""
        if batch_size is None:
            batch_size = DB_CONFIG["batch_size"]
        
        self.log_section_start(f"ğŸš€ å¼€å§‹å¤„ç† [{market_name}] è‚¡ç¥¨æ•°æ®")
        logger.info(f"ğŸ“Š å·²å­˜åœ¨è‚¡ç¥¨æ•°é‡: {len(existing_symbols)}")
        
        start_time = time.time()

        # è·å–åŸå§‹æ•°æ®
        raw_dataframe = self.safely_get_market_data(get_data_func, market_name)
        if raw_dataframe is None or raw_dataframe.empty:
            logger.error(f"âŒ [{market_name}] æ‹‰å–åˆ°çš„æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æœ¬æ¬¡å¸‚åœºå¤„ç†ï¼")
            self.log_section_end(f"[{market_name}] å¤„ç†ç»“æŸ")
            return

        logger.info(f"ğŸ“¥ [{market_name}] åŸå§‹æ•°æ®è¡Œæ•°: {len(raw_dataframe)}")
        
        # æ¸…æ´—æ•°æ®
        cleaned_dataframe = clean_func(raw_dataframe)
        cleaned_dataframe = cleaned_dataframe.dropna(subset=["symbol", "datetime"])
        logger.info(f"ğŸ§¹ [{market_name}] æ¸…æ´—åæ•°æ®è¡Œæ•°: {len(cleaned_dataframe)}")
        
        # å†…å­˜ä¼˜åŒ–ï¼šé‡Šæ”¾åŸå§‹æ•°æ®
        del raw_dataframe
        
        # å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—å¤„ç†å¤§æ•°æ®é›†
        chunk_size = 10000  # æ¯æ¬¡å¤„ç†10000æ¡è®°å½•
        total_rows = len(cleaned_dataframe)
        
        if total_rows > chunk_size:
            logger.info(f"ğŸ“¦ [{market_name}] å¤§æ•°æ®é›†æ£€æµ‹ï¼Œå¯ç”¨åˆ†å—å¤„ç†æ¨¡å¼ (chunk_size={chunk_size})")
            
            exist_records = []
            new_records = []
            new_symbols = set()
            
            # åˆ†å—å¤„ç†
            for i in range(0, total_rows, chunk_size):
                chunk_end = min(i + chunk_size, total_rows)
                chunk_df = cleaned_dataframe.iloc[i:chunk_end]
                
                logger.info(f"ğŸ“¦ [{market_name}] å¤„ç†æ•°æ®å— {i//chunk_size + 1}/{(total_rows-1)//chunk_size + 1} ({i+1}-{chunk_end})")
                
                chunk_records = chunk_df.to_dict(orient="records")
                chunk_symbols = set(chunk_df["symbol"].unique())
                new_symbols.update(chunk_symbols)
                
                # åˆ†ç±»è®°å½•
                chunk_exist = [r for r in chunk_records if r["symbol"] in existing_symbols]
                chunk_new = [r for r in chunk_records if r["symbol"] not in existing_symbols]
                
                exist_records.extend(chunk_exist)
                new_records.extend(chunk_new)
                
                # æ¸…ç†ä¸´æ—¶å˜é‡
                del chunk_df, chunk_records, chunk_symbols, chunk_exist, chunk_new
        else:
            # å°æ•°æ®é›†ç›´æ¥å¤„ç†
            records = cleaned_dataframe.to_dict(orient="records")
            new_symbols = set(cleaned_dataframe["symbol"].unique())
            
            # åˆ†ç±»è®°å½•
            exist_records = [r for r in records if r["symbol"] in existing_symbols]
            new_records = [r for r in records if r["symbol"] not in existing_symbols]
            
            # æ¸…ç†ä¸´æ—¶å˜é‡
            del records
        
        # æ¸…ç†DataFrame
        del cleaned_dataframe
        
        removed_symbols = existing_symbols - new_symbols

        logger.info("-" * 40)
        logger.info(f"ğŸ“ˆ [{market_name}] æ•°æ®ç»Ÿè®¡")
        logger.info(f"ğŸ†• æ–°å¢è®°å½•: {len(new_records)} æ¡")
        logger.info(f"ğŸ”„ æ›´æ–°è®°å½•: {len(exist_records)} æ¡")
        logger.info(f"ğŸ—‘ï¸  åˆ é™¤è®°å½•: {len(removed_symbols)} æ¡")
        logger.info("-" * 40)

        # å¤„ç†æ•°æ®åº“æ“ä½œ
        engine = self.get_db_engine()

        result = self.process_market_records(
            engine,
            market_name,
            exist_records,
            new_records,
            removed_symbols,
            batch_size=batch_size,
            concurrency=concurrency,
        )

        duration = time.time() - start_time
        logger.info("-" * 40)
        logger.info(f"ğŸ‰ [{market_name}] å¤„ç†å®Œæˆï¼")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {self.format_duration(duration)}")
        logger.info(f"ğŸ“Š å¤„ç†ç»“æœ: æ›´æ–° {result['updated']} æ¡, æ–°å¢ {result['inserted']} æ¡, åˆ é™¤ {result['deleted']} æ¡")
        
        failed_updates = result.get("failed_updates") or []
        if failed_updates:
            logger.warning(f"âš ï¸  æ›´æ–°å¤±è´¥: {len(failed_updates)} æ¡")
            logger.warning(f"[{market_name}] æ›´æ–°å¤±è´¥è‚¡ç¥¨ï¼š{sorted(set(failed_updates))}")
        logger.info("-" * 40)
    
    def collect_all_stock_data(self, existing_symbols_map: Dict[str, Set[str]]) -> Dict[str, int]:
        """æ”¶é›†æ‰€æœ‰å¸‚åœºçš„è‚¡ç¥¨æ•°æ®"""
        logger.info("-" * 50)
        logger.info(f"ğŸ“‹ å‡†å¤‡å¤„ç† {len(self.market_configs)} ä¸ªå¸‚åœº")
        logger.info("-" * 50)

        processed_markets = 0
        skipped_markets = 0
        
        for conf in self.market_configs:
            market_name = conf["market_name"]
            if check_market_open_status(conf["check_open_func"], 2):
                logger.info(f"ğŸŸ¢ [{market_name}] å½“å‰æˆ–ä¸¤å°æ—¶å‰ä¸ºå¼€ç›˜æ—¶é—´ï¼Œå¼€å§‹å¤„ç†...")
                existing_symbols = existing_symbols_map.get(market_name, set())
                self.process_and_sync_stock_data(
                    market_name,
                    conf["get_data_func"],
                    conf["clean_func"],
                    existing_symbols,
                    concurrency=conf["concurrency"],
                )
                processed_markets += 1
            else:
                logger.info(f"ğŸ”´ [{market_name}] å½“å‰åŠä¸¤å°æ—¶å‰å‡éå¼€ç›˜æ—¶é—´ï¼Œè·³è¿‡å¤„ç†")
                skipped_markets += 1
        
        return {
            "processed": processed_markets,
            "skipped": skipped_markets
        }
    
    def get_db_engine(self):
        """è·å–æ•°æ®åº“å¼•æ“ï¼ˆä¼˜å…ˆä½¿ç”¨è¿æ¥æ± ï¼‰"""
        if get_db_engine is not None:
            # ä½¿ç”¨è¿æ¥æ± ç®¡ç†å™¨
            return get_db_engine()
        else:
            # é™çº§åˆ°åŸæœ‰æ–¹å¼
            return self._create_legacy_engine()
    
    def _create_legacy_engine(self):
        """åˆ›å»ºä¼ ç»Ÿæ•°æ®åº“å¼•æ“ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        user = os.environ.get("PG_USER")
        password = os.environ.get("PG_PASSWORD")
        host = os.environ.get("PG_HOST")
        port = os.environ.get("PG_PORT", "5432")
        dbname = os.environ.get("PG_DB")
        conn_str = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}"
        logger.info(f"æ„é€ æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²æˆåŠŸ: user={user}, host={host}, port={port}, dbname={dbname}")
        # ä½¿ç”¨ç»Ÿä¸€çš„è¿æ¥æ± ç®¡ç†
        return get_db_engine()
    
    def process_market_records(
        self,
        engine,
        market_name: str,
        exist_records,
        new_records,
        removed_symbols,
        batch_size=1500,
        concurrency=2,
    ):
        """å¤„ç†å¸‚åœºè®°å½•çš„æ‰¹é‡æ“ä½œ"""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        failed_update_symbols = []

        total_updated = 0
        total_update_duration = 0.0
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [
                executor.submit(update_stock_records_batch, engine, exist_records[i:i+batch_size], idx+1, market_name, failed_update_symbols)
                for idx, i in enumerate(range(0, len(exist_records), batch_size))
            ]
            for future in as_completed(futures):
                updated, duration, error = future.result()
                if error is None:
                    total_updated += updated
                    total_update_duration += duration

        logger.info(f"[{market_name}] æ‰¹é‡æ›´æ–°å®Œæˆï¼Œæ›´æ–°æ¡æ•°ï¼š{total_updated}ï¼Œä»»åŠ¡æ€»è€—æ—¶ï¼š{self.format_duration(total_update_duration)}")

        total_inserted = 0
        total_insert_duration = 0.0
        if new_records:
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [
                    executor.submit(insert_stock_records_batch, engine, new_records[i:i+batch_size], idx+1, market_name)
                    for idx, i in enumerate(range(0, len(new_records), batch_size))
                ]
                for future in as_completed(futures):
                    inserted, duration, error = future.result()
                    if error is None:
                        total_inserted += inserted
                        total_insert_duration += duration
            logger.info(f"[{market_name}] æ‰¹é‡æ’å…¥å®Œæˆï¼Œæ’å…¥æ¡æ•°ï¼š{total_inserted}ï¼Œä»»åŠ¡æ€»è€—æ—¶ï¼š{self.format_duration(total_insert_duration)}")
        else:
            logger.info(f"[{market_name}] æ— æ–°å¢è‚¡ç¥¨æ’å…¥")

        deleted_count = 0
        del_duration = 0.0
        if removed_symbols:
            deleted_count, del_duration = mark_stocks_as_deleted(engine, removed_symbols, market_name)
        else:
            logger.info(f"[{market_name}] æ— éœ€æ ‡è®°åˆ é™¤è‚¡ç¥¨")

        return {
            "updated": total_updated,
            "update_time": total_update_duration,
            "inserted": total_inserted,
            "insert_time": total_insert_duration,
            "deleted": deleted_count,
            "delete_time": del_duration,
            "failed_updates": failed_update_symbols,
        }